{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fashion MNIST Dataset Convolutional Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the dataset to be usable in pytorch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "mnist_datasets = datasets.FashionMNIST(root='./data',train=True,download=True,transform=torchvision.transforms.ToTensor())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 256\n",
    "\n",
    "train_iter = DataLoader(mnist_datasets,batch_size,shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "X,y = next(iter(train_iter))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 28, 28])\n",
      "tensor([3, 5, 8, 8, 0, 3, 3, 2, 5, 4, 4, 6, 9, 6, 6, 4, 5, 3, 4, 8, 9, 8, 8, 7,\n",
      "        4, 2, 8, 0, 7, 2, 0, 3, 7, 7, 9, 0, 5, 3, 9, 0, 7, 8, 7, 3, 7, 1, 1, 2,\n",
      "        3, 3, 4, 4, 7, 3, 4, 8, 5, 1, 8, 9, 6, 5, 8, 1, 2, 5, 1, 5, 5, 6, 4, 8,\n",
      "        6, 2, 2, 9, 7, 0, 2, 7, 4, 5, 7, 7, 9, 8, 3, 6, 4, 0, 9, 3, 8, 7, 5, 1,\n",
      "        7, 4, 2, 0, 6, 0, 8, 0, 7, 6, 5, 1, 1, 3, 5, 5, 9, 6, 2, 1, 9, 7, 9, 1,\n",
      "        8, 8, 9, 9, 3, 3, 1, 3, 2, 9, 3, 6, 2, 6, 8, 0, 2, 7, 6, 0, 4, 3, 3, 7,\n",
      "        1, 5, 3, 1, 7, 6, 4, 6, 7, 4, 3, 3, 4, 2, 7, 7, 7, 8, 2, 1, 1, 7, 3, 1,\n",
      "        9, 5, 3, 3, 6, 9, 2, 9, 6, 0, 4, 6, 9, 5, 9, 6, 0, 2, 3, 1, 4, 2, 0, 4,\n",
      "        4, 3, 4, 4, 1, 6, 8, 3, 8, 0, 6, 6, 8, 3, 1, 8, 6, 8, 7, 0, 7, 3, 8, 9,\n",
      "        0, 0, 0, 1, 3, 2, 3, 4, 1, 4, 2, 5, 0, 3, 6, 4, 9, 6, 4, 6, 9, 9, 5, 4,\n",
      "        2, 0, 8, 8, 5, 5, 6, 6, 1, 4, 4, 1, 7, 0, 6, 5])\n"
     ]
    }
   ],
   "source": [
    "print(X.size())\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example Image and corresponding label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x26402e1d2a0>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[5].squeeze(),cmap='gray')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Basic Uses the LeNet Architecture\n",
    "Could try a different architecture and use to convolution layers size by side before pooling for down sampling\n",
    "\n",
    "- Worry about the weight and bias initialization in order to prevent over fitting\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self,num_channels,num_outputs):\n",
    "        super(CNN,self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        #Create the first convolution -> ReLu -> Pool layer (note the structure of this can be changed)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=num_channels,out_channels=6,kernel_size=5,padding=1,stride=1)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.maxpool1 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        #Create the Second convolution,relu, pooling layer\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=6,out_channels=16,kernel_size=5)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.maxpool2 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        #initialize the only fully connected -> ReLu layer\n",
    "        self.linear = torch.nn.Linear(in_features=256,out_features=84)\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "\n",
    "        #Initialize a second fully connected -> ReLu layer\n",
    "        self.linear1 = torch.nn.Linear(in_features=84,out_features=16)\n",
    "        self.relu4 = torch.nn.ReLU()\n",
    "\n",
    "        #Final Softmax stage for classification\n",
    "        self.linear2 = torch.nn.Linear(in_features=16,out_features=num_outputs)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #Pass the input image into the first convolutional layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        #Pass the input image into the second convolutional layer\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        #Pass through the 1st fully connected layer\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.linear(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        #Pass through the 2nd fully connected layer\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        #Pass the output to the softmax classifier to get the output\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu2): ReLU()\n",
      "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear): Linear(in_features=256, out_features=84, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (linear1): Linear(in_features=84, out_features=16, bias=True)\n",
      "  (relu4): ReLU()\n",
      "  (linear2): Linear(in_features=16, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CNN(num_channels=1,num_outputs=10)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the loss and the optimizer functionality"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# Crete the loss component\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Create the optimizer term\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=2.1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create a metric for calculating the Accuracy of the network on the training data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def accuracy(y_hat,y):\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "        cmp = (y_hat.type(y.dtype) == y)\n",
    "        return float(torch.sum(cmp))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a function for training the network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def train_model(model,loss,optimizer,X,y):\n",
    "    model.train()\n",
    "\n",
    "    num_epochs = 300\n",
    "    for epoch in range(num_epochs):\n",
    "        y_hat = model(X)\n",
    "        l = loss(y_hat,y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        l.backward()\n",
    "        optimizer.step()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "train_model(model,loss,optimizer,X,y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "0.79296875"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(X),y)/len(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
